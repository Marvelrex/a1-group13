> **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 13               |
| ----------------------- |
| Student 1 Ragib Sina    |
| Student 2 Heemin Kang   |
| Student 3 Jialin Yang   |
| Student 4 Suchet Mangat |

## Table of Contents

1. [Introduction](#introduction)
2. [High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)
3. [Comparison of exploratory and manual functional testing](#comparison-of-exploratory-and-manual-functional-testing)
4. [Reports](#reports)
5. [Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)
6. [How the pair testing was managed and team work/effort was divided](#how-the-pair-testing-was-managed-and-team-workeffort-was-divided)
7. [Difficulties encountered, challenges overcome, and lessons learned](#difficulties-encountered-challenges-overcome-and-lessons-learned)
8. [Comments/feedback on the lab and lab document itself](#commentsfeedback-on-the-lab-and-lab-document-itself)

## Introduction

<a name="introduction"></a>
In this lab, we will perform exploratory and scripted tests on the ATM simulation system, version 1.0. The required system files are included in the zip archive titled “assignment1-artifacts.zip,” specifically within the file “ATM System - Lab 1 Version 1.0.jar.”
Any bugs identified during exploratory or scripted testing will be logged on the Jira Bug Tracking board. The backlog is accessible via the following link:

https://seng637g13.atlassian.net/jira/software/projects/SEN/boards/1.

Once bugs are identified, we will conduct regression testing on the updated version of the ATM system, which is also included in the same zip archive under the file name “ATM System - Lab 1 Version 1.1.jar.”
Before this lab, we had limited understanding of exploratory and scripted testing, including the distinctions between them. Our previous testing experience was limited to unstructured testing of our own programs in other courses, where we informally verified whether features worked as expected.

## High-level description of the exploratory testing plan

<a name="high-level-description-of-the-exploratory-testing-plan"></a>
The following points outline the high-level exploratory testing plan followed by each pair:

### Approach

- Focus on exploring common routines that a typical user would perform when interacting with the ATM.
- Investigate potential user actions, such as pressing incorrect buttons or performing unintended operations.
- Explore all available functionalities to some extent.
- After completing each transaction, verify the account balance and logs to ensure all actions are recorded correctly.
- Examine the receipts generated to confirm they include accurate details.

### Targeted Functionalities

- Viewing the balance of available accounts.
- Depositing cash into accounts.
- Withdrawing cash from accounts.
- Transferring money between accounts.
- Verifying the transaction logs for accuracy and completeness.

## Comparison of exploratory and manual functional testing

<a name="comparison-of-exploratory-and-manual-functional-testing"></a>

Our team discovered significantly more bugs during exploratory testing compared to scripted testing sessions. This difference is likely due to each team member approaching the software in unique and innovative ways, which allowed them to uncover diverse use cases that were not addressed in the 40 scripted test cases. The primary advantage of exploratory testing is its flexibility; testers are not confined to predefined steps, enabling them to identify unexpected issues. This approach is particularly effective when scripted tests are not exhaustive, as was the case with our testing, where the 40 manual scripted tests failed to cover many of the bugs.

However, exploratory testing also presents challenges. The lack of a structured plan made it difficult to track testing progress systematically. Additionally, some bugs may have been overlooked or left unconfirmed, as reproducing an issue without predefined steps was challenging in certain instances.

In contrast, scripted testing offers better progress tracking and bug documentation. The predefined steps ensure that anyone following the test cases can consistently verify whether the code meets the minimum quality standards. Once written, scripted tests do not depend on the tester’s intuition, ensuring a structured and repeatable testing process.

Nevertheless, scripted testing has its limitations, particularly its reliance on the foresight of the test case writer. If a test case fails to account for a potential issue, that bug is likely to remain undetected in all rounds of testing.

Interestingly, some bugs initially discovered through exploratory testing were later found to be included in the scripted test list. These included MFT #14, 22, and 29, confirming that both testing methods have their own merits and should be used complementarity to ensure thorough software quality assurance.

## Reports

<a name="reports"></a>

For this assignment, we have included two distinct reports in the /reports folder. The first report is a table that tracks all 40 MFT cases, and it provides links to the corresponding bug reports in Jira if a bug is identified.

The second report is generated from Jira, our chosen bug tracking software. This report compiles bugs discovered across all versions of the software, capturing both exploratory testing and manual scripted testing results. It features a column labeled "Defect Source Test" that specifies the testing method that identified the bug. Bugs found through both manual scripted testing and exploratory testing are tagged accordingly. Additionally, any new bugs identified only in version 1.1 that were not included in the MFT list are marked with a "To-Do" status in the report.

## Notes and discussion of the peer reviews of defect reports

<a name="notes-and-discussion-of-the-peer-reviews-of-defect-reports"></a>
During exploratory testing, we ensured that there were no repeated bug reports, which gave each team member a comprehensive view of functional defects across the system. During regression testing, we used a systematic approach to randomly assign defects found in version 1.0 that were in a “to-do” status to each team member for subsequent retesting. This collaborative process enhanced knowledge sharing among team members and created opportunities for constructive feedback and suggestions.

## How the pair testing was managed and team work/effort was divided

<a name="how-the-pair-testing-was-managed-and-team-workeffort-was-divided"></a>
For exploratory testing, each team member conducted individual testing sessions lasting approximately 30 minutes, excluding the time spent recording results. During this process, we documented all identified bugs. Once the testing was completed, we compiled the findings of each team member into a consolidated bug list. We then tested the identified bugs in version 1.1 of the system under test (SUT) to verify whether they persisted. To ensure no new issues were introduced, we also re-tested general functionality, even though most areas had already been addressed due to the high number of bugs found in version 1.0.

For milestone function testing (MFT) and regression testing, we divided the forty scripted test cases evenly between two pairs. Ragib and Jialin tested cases 1-20, while Suchet and Heemin tested cases 21-40. Within each pair, one member performed the tests while the other documented the steps and recorded any bugs. This approach ensured accurate tracking and validation of test results. For each test case, we recorded our observations in version 1.0 and then noted comparative observations in version 1.1 of the SUT.

Finally, the two pairs conducted a peer review of the MFT and regression testing results. This involved cross-checking records from both versions (1.0 and 1.1) of the SUT to determine if the documented issues were consistently reproducible within the ATM program.

## Difficulties encountered, challenges overcome, and lessons learned

<a name="difficulties-encountered-challenges-overcome-and-lessons-learned"></a>
One of the biggest challenges was finding similar bugs in different parts of the system. For example, after completing a transaction, the system prompts the user to indicate whether another transaction is needed. In version 1.1, this prompt contained a spelling error. During pair testing, multiple team members found and documented this same issue, which slowed down testing progress. To address this challenge, we created a more granular distribution of test cases using the UML diagrams provided in the assignment guidelines, effectively avoiding duplication of testing efforts.

## Comments/feedback on the lab and lab document itself

<a name="commentsfeedback-on-the-lab-and-lab-document-itself"></a>

We used Jira as our bug reporting and management tool, and we found it to be highly effective for this assignment. It provided a structured approach to tracking issues, making it easier to document, organize, and manage bugs efficiently. Additionally, working with Jira allowed us to simulate a comprehensive testing process that mirrored real-world software development workflows, including bug reporting, status tracking, and regression testing procedures.

The SUT selected for this assignment was an excellent choice for both exploratory and scripted testing, as well as regression testing. It was simple enough for anyone to understand, yet it contained a substantial number of bugs, providing ample opportunities to apply different testing methodologies effectively.
